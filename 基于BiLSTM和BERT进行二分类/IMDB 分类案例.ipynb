{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63437494",
   "metadata": {},
   "source": [
    "### 案例：基于BiLSTM和BERT对数据集IMDB进行分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53977d8",
   "metadata": {},
   "source": [
    "### 流程：\n",
    "\n",
    "#### 第1步：加载IMDB数据集；\n",
    "#### 第2步：对text进行分词；\n",
    "#### 第3步：基于tf.py_function函数封装一个自定义的函数处理padding；\n",
    "#### 第4步：加载预训练的 GloVe embeddings；\n",
    "#### 第5步：基于GloVe创建IMDB的embedding；\n",
    "#### 第6步：基于词袋定义BiLSTM模型；\n",
    "#### 第7步：基于BERT实现分类；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c1ec06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91851734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2be5cf6d",
   "metadata": {},
   "source": [
    "### 第1步：加载 IMDB 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93b19bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbe57523",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-26 14:32:57.729853: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2021-12-26 14:32:57.729875: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: tgl\n",
      "2021-12-26 14:32:57.729879: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: tgl\n",
      "2021-12-26 14:32:57.729924: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.86.0\n",
      "2021-12-26 14:32:57.729938: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.86.0\n",
      "2021-12-26 14:32:57.729942: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.86.0\n",
      "2021-12-26 14:32:57.730696: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "train_dataset, info = tfds.load(name='imdb_reviews', # 数据集名称\n",
    "                                split='train', # 切分为训练集\n",
    "                                with_info=True, # 数据集信息\n",
    "                                as_supervised=True # 返回 (input, label)\n",
    "                               )\n",
    "\n",
    "# 参考API用法：https://tensorflow.google.cn/datasets/api_docs/python/tfds/load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c5dde04",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = tfds.load(name='imdb_reviews',\n",
    "                         split='test',\n",
    "                         as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3771bb38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='imdb_reviews',\n",
       "    full_name='imdb_reviews/plain_text/1.0.0',\n",
       "    description=\"\"\"\n",
       "    Large Movie Review Dataset.\n",
       "    This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.\n",
       "    \"\"\",\n",
       "    config_description=\"\"\"\n",
       "    Plain text\n",
       "    \"\"\",\n",
       "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
       "    data_path='/home/tgl/tensorflow_datasets/imdb_reviews/plain_text/1.0.0',\n",
       "    download_size=80.23 MiB,\n",
       "    dataset_size=129.83 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "        'text': Text(shape=(), dtype=tf.string),\n",
       "    }),\n",
       "    supervised_keys=('text', 'label'),\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=25000, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=25000, num_shards=1>,\n",
       "        'unsupervised': <SplitInfo num_examples=50000, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
       "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
       "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
       "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
       "      month     = {June},\n",
       "      year      = {2011},\n",
       "      address   = {Portland, Oregon, USA},\n",
       "      publisher = {Association for Computational Linguistics},\n",
       "      pages     = {142--150},\n",
       "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info # 数据集信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8841b39d",
   "metadata": {},
   "source": [
    "### 第2步：对text进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a06dfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds_tokenizer = tfds.deprecated.text.Tokenizer()   # 默认的tokenizer\n",
    "\n",
    "# 参考API用法：https://www.tensorflow.org/datasets/api_docs/python/tfds/deprecated/text/Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ac51790",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_set = set() # 词库表\n",
    "MAX_LEN = 0  # 句子的最大长度\n",
    "\n",
    "\n",
    "for text, label in train_dataset:\n",
    "    tokens = tfds_tokenizer.tokenize(text.numpy())  # 对text进行分词\n",
    "    if MAX_LEN < len(tokens):\n",
    "        MAX_LEN = len(tokens)  # 获取最长的句子\n",
    "    vocab_set.update(tokens)  # 用于修改当前集合，可以添加新的元素或集合到当前集合中，如果添加的元素在集合中已存在，则该元素只会出现一次，重复的会忽略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9e67720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2525"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46a3ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于上述 tokenizer 和 vocab_set 创建一个 text encoder\n",
    "\n",
    "encoder = tfds.deprecated.text.TokenTextEncoder(vocab_list=vocab_set, # 词汇表\n",
    "                                                lowercase=True, # 全部小写\n",
    "                                                tokenizer=tfds_tokenizer # 分词器\n",
    "                                               )\n",
    "\n",
    "# 参考API用法：https://www.tensorflow.org/datasets/api_docs/python/tfds/deprecated/text/TokenTextEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e12313e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93931"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.vocab_size  # 不同的词汇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7404b29e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNK'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.oov_token  # 未出现的词汇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21ee10a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tilton',\n",
       " 'frantisek',\n",
       " 'catastrophic',\n",
       " 'deduces',\n",
       " 'prematurely',\n",
       " 'dumbrille',\n",
       " 'unneeded',\n",
       " 'inserted',\n",
       " 'movieworld',\n",
       " 'crutchley']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.tokens[:10]  # 词汇"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d117cf0",
   "metadata": {},
   "source": [
    "### 第3步：基于tf.py_function函数封装一个自定义的函数处理padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2ea860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def encode_padding(text):\n",
    "    encode_result = encoder.encode(text.numpy())  # 对text进行分词，并转换为int类型的数值\n",
    "    pad_result = pad_sequences([encode_result],   # 填充\n",
    "                               padding='post',\n",
    "                               truncating='post',\n",
    "                               maxlen=150)\n",
    "    return np.array(pad_result[0], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8f4411a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([88497, 70219, 71769, 93729, 59938, 71769,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 示例\n",
    "\n",
    "encode_padding(tf.constant(b\"Today is Christmas Day. Merry Christmas.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f731b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_function(text, label):\n",
    "    encode_func = tf.py_function(encode_padding, # 函数名\n",
    "                                 inp=[text], # 输入参数\n",
    "                                 Tout=tf.int64 # 返回值类型\n",
    "                                )\n",
    "    return encode_func, label\n",
    "\n",
    "# 参考API用法：https://www.tensorflow.org/api_docs/python/tf/py_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "583e8c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对 train_dataset 和 test_dataset 进行 padding\n",
    "\n",
    "encoded_train = train_dataset.map(encode_function,\n",
    "                                  num_parallel_calls=tf.data.experimental.AUTOTUNE # 输入元素彼此独立，因此预处理可以跨多个CPU内核并行化\n",
    "                                 )\n",
    "\n",
    "# 参考API用法：https://www.tensorflow.org/api_docs/python/tf/data/experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1f4cadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test = test_dataset.map(encode_function,\n",
    "                                  num_parallel_calls=tf.data.experimental.AUTOTUNE # 输入元素彼此独立，因此预处理可以跨多个CPU内核并行化\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fb1c51",
   "metadata": {},
   "source": [
    "### 第4步：加载预训练的 GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8be4f951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先提前下载 glove \n",
    "\n",
    "!wget https://nlp.stanford.edu/data/glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "292b93c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解压\n",
    "\n",
    "!unzip glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "614d5ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文件 glove.6B.50d.txt\n",
    "\n",
    "word_embedding = dict()\n",
    "\n",
    "with open('glove.6B.50d.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        tokens = line.split() # 根据空格分词\n",
    "        word = tokens[0] # 单词\n",
    "        vector = np.array(tokens[1:], dtype=np.float32) # 向量表示\n",
    "        if vector.shape[0] == 50: \n",
    "            word_embedding[word] = vector\n",
    "        else:\n",
    "            print(\"wrong word embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df14716c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_embedding)  # 40万个单词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d346612",
   "metadata": {},
   "source": [
    "### 第5步：基于GloVe创建IMDB的embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f51d428",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 50\n",
    "\n",
    "emb_matrix = np.zeros((encoder.vocab_size, dim))  # 初始化一个matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b64079da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93931, 50)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f2cf0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_word_count = 0\n",
    "unknown_word = set()\n",
    "\n",
    "for word in encoder.tokens: # 获取每一个词\n",
    "    vector = word_embedding.get(word)  # 根据词获取对应的embedding,如果没有，则返回None\n",
    "    \n",
    "    if vector is not None:\n",
    "        idx = encoder.encode(word)[0] # 单词对应的索引\n",
    "        emb_matrix[idx] = vector  # 获取到GloVe的词向量\n",
    "    else:\n",
    "        unknown_word_count += 1  # 没有匹配到的词\n",
    "        unknown_word.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2929a906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14553"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfe0fb7",
   "metadata": {},
   "source": [
    "### 第6步：基于词袋定义BiLSTM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17ba6915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 超参数定义\n",
    "\n",
    "vocab_size = encoder.vocab_size  # 词表大小\n",
    "\n",
    "units = 64  # 神经元数量\n",
    "\n",
    "batch_size = 100  # 批次大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0ae72b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义BiLSTM模型\n",
    "from tensorflow.keras.layers import Layer, Embedding, LSTM, Bidirectional, Dense\n",
    "\n",
    "def build_bilstm_model(vocab_size, embedding_dim, units, batch_size, train = False):\n",
    "    model = tf.keras.Sequential([\n",
    "        Embedding(vocab_size,           # 词表大小\n",
    "                  embedding_dim,        # 词向量维度\n",
    "                  mask_zero=True,       # mask用0填充\n",
    "                  weights=[emb_matrix], # 用GloVe预训练词向量\n",
    "                  trainable=train),     # 设置为False，防止在训练过程中更新参数\n",
    "        Bidirectional(LSTM(units, return_sequences=True, dropout=0.5)),\n",
    "        Bidirectional(LSTM(units, dropout=0.25)),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# 参考API用法：https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7826086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模型\n",
    "\n",
    "model = build_bilstm_model(vocab_size, embedding_dim=dim, units=units, batch_size=batch_size, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98be9e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 50)          4696550   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, None, 128)        58880     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 128)              98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,854,375\n",
      "Trainable params: 157,825\n",
      "Non-trainable params: 4,696,550\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8f05c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型编译\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d845ee2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "250/250 [==============================] - 44s 147ms/step - loss: 0.6084 - accuracy: 0.6655 - precision: 0.6700 - recall: 0.6522\n",
      "Epoch 2/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.5443 - accuracy: 0.7232 - precision: 0.7231 - recall: 0.7235\n",
      "Epoch 3/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.5398 - accuracy: 0.7295 - precision: 0.7265 - recall: 0.7361\n",
      "Epoch 4/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.5040 - accuracy: 0.7544 - precision: 0.7566 - recall: 0.7502\n",
      "Epoch 5/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.4995 - accuracy: 0.7571 - precision: 0.7524 - recall: 0.7666\n",
      "Epoch 6/30\n",
      "250/250 [==============================] - 38s 150ms/step - loss: 0.4700 - accuracy: 0.7748 - precision: 0.7820 - recall: 0.7620\n",
      "Epoch 7/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.4668 - accuracy: 0.7787 - precision: 0.7752 - recall: 0.7850\n",
      "Epoch 8/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.4665 - accuracy: 0.7783 - precision: 0.7788 - recall: 0.7773\n",
      "Epoch 9/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.4868 - accuracy: 0.7656 - precision: 0.7593 - recall: 0.7779\n",
      "Epoch 10/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.4645 - accuracy: 0.7823 - precision: 0.7785 - recall: 0.7892\n",
      "Epoch 11/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.4519 - accuracy: 0.7916 - precision: 0.7883 - recall: 0.7974\n",
      "Epoch 12/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.4353 - accuracy: 0.7946 - precision: 0.7907 - recall: 0.8012\n",
      "Epoch 13/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.4267 - accuracy: 0.8007 - precision: 0.7975 - recall: 0.8060\n",
      "Epoch 14/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.4232 - accuracy: 0.8017 - precision: 0.7976 - recall: 0.8086\n",
      "Epoch 15/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.4096 - accuracy: 0.8125 - precision: 0.8067 - recall: 0.8221\n",
      "Epoch 16/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.4022 - accuracy: 0.8142 - precision: 0.8122 - recall: 0.8174\n",
      "Epoch 17/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.4019 - accuracy: 0.8160 - precision: 0.8139 - recall: 0.8195\n",
      "Epoch 18/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.3944 - accuracy: 0.8172 - precision: 0.8165 - recall: 0.8182\n",
      "Epoch 19/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.3999 - accuracy: 0.8164 - precision: 0.8147 - recall: 0.8190\n",
      "Epoch 20/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.3897 - accuracy: 0.8223 - precision: 0.8204 - recall: 0.8252\n",
      "Epoch 21/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.3817 - accuracy: 0.8267 - precision: 0.8244 - recall: 0.8302\n",
      "Epoch 22/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.3781 - accuracy: 0.8286 - precision: 0.8248 - recall: 0.8343\n",
      "Epoch 23/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.3750 - accuracy: 0.8292 - precision: 0.8283 - recall: 0.8307\n",
      "Epoch 24/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.3746 - accuracy: 0.8276 - precision: 0.8264 - recall: 0.8293\n",
      "Epoch 25/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.3694 - accuracy: 0.8327 - precision: 0.8301 - recall: 0.8366\n",
      "Epoch 26/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.3669 - accuracy: 0.8328 - precision: 0.8312 - recall: 0.8353\n",
      "Epoch 27/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.3605 - accuracy: 0.8372 - precision: 0.8357 - recall: 0.8396\n",
      "Epoch 28/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.3584 - accuracy: 0.8385 - precision: 0.8344 - recall: 0.8446\n",
      "Epoch 29/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.3554 - accuracy: 0.8400 - precision: 0.8375 - recall: 0.8438\n",
      "Epoch 30/30\n",
      "250/250 [==============================] - 37s 148ms/step - loss: 0.3502 - accuracy: 0.8415 - precision: 0.8383 - recall: 0.8463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f57e3a83d60>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型训练\n",
    "\n",
    "train_batch_dataset = encoded_train.batch(batch_size).prefetch(100)  # CPU预先加载数据集\n",
    "\n",
    "model.fit(train_batch_dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "424f8e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 15s 51ms/step - loss: 0.4108 - accuracy: 0.8374 - precision: 0.7979 - recall: 0.9037\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.41081348061561584,\n",
       " 0.8373600244522095,\n",
       " 0.7978528141975403,\n",
       " 0.9036800265312195]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型验证\n",
    "\n",
    "model.evaluate(encoded_test.batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f5e9f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化：针对 word 为0的向量进行训练\n",
    "\n",
    "model_v2 = build_bilstm_model(vocab_size=vocab_size,\n",
    "                              embedding_dim=dim,\n",
    "                              units=units,\n",
    "                              batch_size=batch_size,\n",
    "                              train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "696d6c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 50)          4696550   \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, None, 128)        58880     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 128)              98816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,854,375\n",
      "Trainable params: 4,854,375\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_v2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c65d9c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型编译\n",
    "\n",
    "model_v2.compile(loss='binary_crossentropy',\n",
    "                 optimizer='adam',\n",
    "                 metrics=['accuracy', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6e795f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "250/250 [==============================] - 51s 174ms/step - loss: 0.5824 - accuracy: 0.6861 - precision: 0.6842 - recall: 0.6912\n",
      "Epoch 2/30\n",
      "250/250 [==============================] - 44s 174ms/step - loss: 0.5117 - accuracy: 0.7500 - precision: 0.7486 - recall: 0.7530\n",
      "Epoch 3/30\n",
      "250/250 [==============================] - 43s 174ms/step - loss: 0.4157 - accuracy: 0.8132 - precision: 0.8082 - recall: 0.8213\n",
      "Epoch 4/30\n",
      "250/250 [==============================] - 44s 174ms/step - loss: 0.3624 - accuracy: 0.8468 - precision: 0.8355 - recall: 0.8636\n",
      "Epoch 5/30\n",
      "250/250 [==============================] - 44s 174ms/step - loss: 0.3101 - accuracy: 0.8696 - precision: 0.8697 - recall: 0.8694\n",
      "Epoch 6/30\n",
      "250/250 [==============================] - 44s 174ms/step - loss: 0.2659 - accuracy: 0.8930 - precision: 0.8941 - recall: 0.8915\n",
      "Epoch 7/30\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.2324 - accuracy: 0.9078 - precision: 0.9056 - recall: 0.9106\n",
      "Epoch 8/30\n",
      "250/250 [==============================] - 43s 174ms/step - loss: 0.1971 - accuracy: 0.9232 - precision: 0.9231 - recall: 0.9232\n",
      "Epoch 9/30\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.1701 - accuracy: 0.9351 - precision: 0.9359 - recall: 0.9342\n",
      "Epoch 10/30\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.1443 - accuracy: 0.9459 - precision: 0.9463 - recall: 0.9454\n",
      "Epoch 11/30\n",
      "250/250 [==============================] - 43s 174ms/step - loss: 0.1261 - accuracy: 0.9545 - precision: 0.9547 - recall: 0.9543\n",
      "Epoch 12/30\n",
      "250/250 [==============================] - 43s 174ms/step - loss: 0.1064 - accuracy: 0.9607 - precision: 0.9623 - recall: 0.9590\n",
      "Epoch 13/30\n",
      "250/250 [==============================] - 43s 174ms/step - loss: 0.0887 - accuracy: 0.9680 - precision: 0.9685 - recall: 0.9675\n",
      "Epoch 14/30\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.0832 - accuracy: 0.9704 - precision: 0.9709 - recall: 0.9700\n",
      "Epoch 15/30\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.0685 - accuracy: 0.9754 - precision: 0.9757 - recall: 0.9750\n",
      "Epoch 16/30\n",
      "250/250 [==============================] - 43s 174ms/step - loss: 0.0604 - accuracy: 0.9788 - precision: 0.9796 - recall: 0.9780\n",
      "Epoch 17/30\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.0576 - accuracy: 0.9801 - precision: 0.9807 - recall: 0.9795\n",
      "Epoch 18/30\n",
      "250/250 [==============================] - 43s 174ms/step - loss: 0.0490 - accuracy: 0.9834 - precision: 0.9834 - recall: 0.9834\n",
      "Epoch 19/30\n",
      "250/250 [==============================] - 43s 174ms/step - loss: 0.0432 - accuracy: 0.9843 - precision: 0.9843 - recall: 0.9842\n",
      "Epoch 20/30\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.0372 - accuracy: 0.9863 - precision: 0.9861 - recall: 0.9866\n",
      "Epoch 21/30\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.0338 - accuracy: 0.9879 - precision: 0.9879 - recall: 0.9879\n",
      "Epoch 22/30\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.0323 - accuracy: 0.9885 - precision: 0.9884 - recall: 0.9886\n",
      "Epoch 23/30\n",
      "250/250 [==============================] - 43s 173ms/step - loss: 0.0320 - accuracy: 0.9885 - precision: 0.9886 - recall: 0.9884\n",
      "Epoch 24/30\n",
      "250/250 [==============================] - 44s 174ms/step - loss: 0.0277 - accuracy: 0.9899 - precision: 0.9909 - recall: 0.9888\n",
      "Epoch 25/30\n",
      "250/250 [==============================] - 44s 177ms/step - loss: 0.0273 - accuracy: 0.9903 - precision: 0.9908 - recall: 0.9898\n",
      "Epoch 26/30\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.0235 - accuracy: 0.9918 - precision: 0.9918 - recall: 0.9918\n",
      "Epoch 27/30\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.0215 - accuracy: 0.9924 - precision: 0.9919 - recall: 0.9928\n",
      "Epoch 28/30\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.0229 - accuracy: 0.9918 - precision: 0.9921 - recall: 0.9914\n",
      "Epoch 29/30\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.0203 - accuracy: 0.9929 - precision: 0.9932 - recall: 0.9926\n",
      "Epoch 30/30\n",
      "250/250 [==============================] - 44s 175ms/step - loss: 0.0185 - accuracy: 0.9932 - precision: 0.9936 - recall: 0.9928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f57166ec100>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型训练\n",
    "\n",
    "model_v2.fit(train_batch_dataset, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1081efdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 15s 52ms/step - loss: 1.0289 - accuracy: 0.8052 - precision: 0.7785 - recall: 0.8532\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.028915286064148, 0.8052399754524231, 0.7785239815711975, 0.8532000184059143]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型验证\n",
    "\n",
    "model_v2.evaluate(encoded_test.batch(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b089e607",
   "metadata": {},
   "source": [
    "### 第7步：基于BERT实现分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7059df6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd1f9406",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_name,                # 预训练模型名称\n",
    "                                          add_special_tokens=True,  # 添加特殊字符，[CLS], [SEP], [PAD]\n",
    "                                          do_lower_case=True,       # 全部转换为小写\n",
    "                                          max_length=150,           # 自定义句子长度\n",
    "                                          pad_to_max_length=True)   # 填充\n",
    "\n",
    "# BERT理论部分，请参考：https://space.bilibili.com/474347248/channel/seriesdetail?sid=856305"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3116c7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1996, 2822, 2047, 2095, 2003, 2746, 1012, 102, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 示例 1 -- 单句\n",
    "\n",
    "tokenizer.encode_plus(\"The Chinese New Year is coming.\",\n",
    "                      add_special_tokens=True,\n",
    "                      max_length=15,\n",
    "                      pad_to_max_length=True,\n",
    "                      return_attention_mask=True,\n",
    "                      return_token_type_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cdd96321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1996, 2822, 2047, 2095, 102, 2009, 2003, 2746, 102, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 示例 2 -- 双句\n",
    "\n",
    "tokenizer.encode_plus(\"The Chinese New Year\", \"it is coming\",\n",
    "                      add_special_tokens=True,\n",
    "                      max_length=15,\n",
    "                      pad_to_max_length=True,\n",
    "                      return_attention_mask=True,\n",
    "                      return_token_type_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57f670b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据上述示例，定义方法，将输入文本进行encode\n",
    "\n",
    "def bert_encode(text):\n",
    "    text = text.numpy().decode('utf-8')  # 先转换为array类型，再进行编码转换\n",
    "    encode_result = tokenizer.encode_plus(text,\n",
    "                                          add_special_tokens=True,\n",
    "                                          max_length=150,\n",
    "                                          pad_to_max_length=True,\n",
    "                                          return_attention_mask=True,\n",
    "                                          return_token_type_ids=True)\n",
    "    input_ids = encode_result['input_ids']\n",
    "    token_type_ids = encode_result['token_type_ids']\n",
    "    attention_mask = encode_result['attention_mask']\n",
    "    \n",
    "    return input_ids, token_type_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c3190a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[50285 84571 82065 62217 84683 87904 53684 63366 85729 56017 52083 75692\n",
      " 43839 91715 73352 49854 25771 78155 89161 82951 86212 68764 50285 78310\n",
      " 67848 85729 61078 42793 91632 52083 57439 85886 61078 82951 58903 90473\n",
      " 73734 52739 50285 87904 93060 79769 93824 50285 87904 70219 82065 65457\n",
      " 59224 76117 64602 60352 84565 73093 83216 31704 53097 66730 88206 84565\n",
      " 17887 75648 53097 52207 61078 66501 62051 72727 30739 23735 53808 58916\n",
      " 70526 45151 77997 86390 91881 84738 57222 91715 84571 69561 68764 43384\n",
      " 83216 90182 12640 52083 43384 87904 69365 84571 89982 91220 89077 57728\n",
      " 86551 14879 63514 55000 69365 92990 89161 90962 89207 50285 79623 89851\n",
      " 93060 89207 43839 91715 93060 67563 74675 14879 90473 92980 71481 54205\n",
      " 53768     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0], shape=(150,), dtype=int64)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# 查看 训练集的一个示例\n",
    "\n",
    "for text, label in encoded_train.take(1):\n",
    "    print(text)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "075137b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对 encode_train 数据集 进行 encode 转换\n",
    "\n",
    "bert_encode_train = [bert_encode(text) for text, label in train_dataset]  # 数据集，包含：input_ids, token_type_ids, attention_mask\n",
    "bert_encode_label = [label for text, label in train_dataset]              # 标签集\n",
    "\n",
    "bert_encode_train = np.array(bert_encode_train)                                      # 类型转换 tensor -> array\n",
    "bert_encode_label = tf.keras.utils.to_categorical(bert_encode_label, num_classes=2)  # 标签类型转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c653e568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 示例\n",
    "\n",
    "tf.keras.utils.to_categorical(tf.constant([0, 1, 1, 0]), num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7fed786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对 encode_train 进行 切分为： train 和 val\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(bert_encode_train, \n",
    "                                                  bert_encode_label, \n",
    "                                                  test_size=0.2, \n",
    "                                                  random_state=666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f87bd640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 3, 150)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6efd6c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "46b1a7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将 X_train 和 X_val 分为三部分\n",
    "\n",
    "train_inputs_ids, train_token_type_ids, train_attention_masks = np.split(X_train, 3, axis=1)  # 拆分\n",
    "\n",
    "val_inputs_ids, val_token_type_ids, val_attention_masks = np.split(X_val, 3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1208a1c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 1, 150)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "efedd133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 1, 150)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_token_type_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cc89fb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 1, 150)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_attention_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4a2d6609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 减掉 1 维\n",
    "\n",
    "train_inputs_ids = train_inputs_ids.squeeze()\n",
    "train_token_type_ids = train_token_type_ids.squeeze()\n",
    "train_attention_masks = train_attention_masks.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b67e3991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 减掉 1 维\n",
    "\n",
    "val_inputs_ids = val_inputs_ids.squeeze()\n",
    "val_token_type_ids = val_token_type_ids.squeeze()\n",
    "val_attention_masks = val_attention_masks.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a0ea7ee9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 150)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6d2f73a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 150)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_token_type_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0826f307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 150)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_attention_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e09cab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 构建 训练和验证批数据\n",
    "\n",
    "def combine_dataset(input_ids, token_type_ids, attention_mask, label):\n",
    "    data_format = {'input_ids' : input_ids,\n",
    "                   'token_type_ids' :token_type_ids,\n",
    "                   'attention_mask' : attention_mask}\n",
    "    \n",
    "    return data_format, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6dd7b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练批数据\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_inputs_ids,\n",
    "                                               train_token_type_ids,\n",
    "                                               train_attention_masks,\n",
    "                                               y_train)).map(combine_dataset).shuffle(100).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1c93ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证批数据\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_inputs_ids,\n",
    "                                             val_token_type_ids,\n",
    "                                             val_attention_masks,\n",
    "                                             y_val)).map(combine_dataset).shuffle(100).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "756fca8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 从Hugging Face加载分类模型\n",
    "\n",
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained(bert_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c9156f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化器\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "\n",
    "\n",
    "# 定义损失函数\n",
    "\n",
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "28f7f2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型编译\n",
    "\n",
    "bert_model.compile(optimizer=optimizer,\n",
    "                   loss=loss,\n",
    "                   metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "94d5476c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  109482240 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  1538      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 109,483,778\n",
      "Trainable params: 109,483,778\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "bert_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c57bd9d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f58a4a92e20>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f58a4a92e20>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f58a4a92e20>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1250/1250 [==============================] - ETA: 0s - loss: 0.3276 - accuracy: 0.8546WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "1250/1250 [==============================] - 4333s 3s/step - loss: 0.3276 - accuracy: 0.8546 - val_loss: 0.2595 - val_accuracy: 0.8894\n"
     ]
    }
   ],
   "source": [
    "history = bert_model.fit(train_ds,\n",
    "                         epochs=1,\n",
    "                         validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5ab0f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在测试集上进行测试\n",
    "\n",
    "bert_test = [bert_encode(text) for text ,label in test_dataset]\n",
    "bert_test_label = [label for text, label in test_dataset]\n",
    "\n",
    "bert_test = np.array(bert_test)\n",
    "bert_test_label = tf.keras.utils.to_categorical(bert_test_label, num_classes=2)\n",
    "\n",
    "test_inputs_ids, test_token_type_ids, test_attention_masks = np.split(bert_test, 3, axis=1)  # 拆分\n",
    "\n",
    "test_inputs_ids = test_inputs_ids.squeeze()\n",
    "test_token_type_ids = test_token_type_ids.squeeze()\n",
    "test_attention_masks = test_attention_masks.squeeze()\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_inputs_ids,\n",
    "                                              test_token_type_ids,\n",
    "                                              test_attention_masks,\n",
    "                                              bert_test_label)).map(combine_dataset).shuffle(100).batch(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6e1abde9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 1490s 953ms/step - loss: 0.2553 - accuracy: 0.8923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25527384877204895, 0.8922799825668335]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型测试\n",
    "\n",
    "bert_model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a50918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
